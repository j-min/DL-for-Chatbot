{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:00:09.646416Z",
     "start_time": "2017-07-30T20:00:09.144299Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import PackedSequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:00:09.730576Z",
     "start_time": "2017-07-30T20:00:09.647973Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchtext import data, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:00:09.903426Z",
     "start_time": "2017-07-30T20:00:09.732476Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from konlpy.tag import Twitter, Mecab\n",
    "twitter = Twitter()\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:00:09.933071Z",
     "start_time": "2017-07-30T20:00:09.905071Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hangul = re.compile(r'[^ ㄱ-ㅣ가-힣.,?!]+')\n",
    "def clean(sentence):\n",
    "    clean_sentence = hangul.sub('', sentence)\n",
    "    return clean_sentence\n",
    "def mecab_tokenizer(sentence):\n",
    "    out_list = []\n",
    "    for word, pos in mecab.pos(sentence):\n",
    "        out_list.append(word)\n",
    "    return out_list\n",
    "def tokenizer(sentence):\n",
    "    clean_sentence = clean(sentence)\n",
    "    tokens = mecab_tokenizer(clean_sentence)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:00:09.963138Z",
     "start_time": "2017-07-30T20:00:09.934526Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕', '하', '세요', '?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('안녕하세요?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo QA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:00:10.003819Z",
     "start_time": "2017-07-30T20:00:09.965561Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q1 = '안녕하세요?'\n",
    "A1 = '안녕하세요!'\n",
    "\n",
    "Q2 = '오늘 날씨가 어떤가요?'\n",
    "A2 = '비가 오네요.'\n",
    "\n",
    "Q3 = '커피 좋아하세요?'\n",
    "A3 = '네, 헤이즐넛 라떼를 좋아해요.'\n",
    "\n",
    "Q4 = '집에서는 주로 뭘 하시나요?'\n",
    "A4 = '자야죠 뭐ㅎㅎ 보통 집에 오래 안 있어요'\n",
    "\n",
    "Q5 = '야 너 어디사냐'\n",
    "A5 = '저는 서울 살아요'\n",
    "\n",
    "Q6 = '여기 자장면 배달 되나요'\n",
    "A6 = '죄송하지만 그런 질문에는 대답할 수 없습니다'\n",
    "\n",
    "Q7 = '너 혹시 뭐 좋아하는 드라마 있어?'\n",
    "A7 = '좀 오래된 거긴 한데 빅뱅이론 좋아해요.'\n",
    "\n",
    "Q8 = '영화 보러가자.'\n",
    "A8 = '영화를 본 지가 좀 오래 되었는데 혹시 요즘 뭐 재밌는 영화가 있나요?'\n",
    "\n",
    "QA_list = [(Q1, A1),\n",
    "           (Q2, A2),\n",
    "           (Q3, A3),\n",
    "           (Q4, A4),\n",
    "           (Q5, A5),\n",
    "           (Q6, A6),\n",
    "           (Q7, A7),\n",
    "           (Q8, A8)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset using torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:00:10.035274Z",
     "start_time": "2017-07-30T20:00:10.005400Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate vocab between question & answer (common in machine translation)\n",
    "# question_field = data.Field(tokenize=tokenizer)\n",
    "# answer_field = data.Field(tokenize=tokenizer)\n",
    "\n",
    "# shared vocab between question & answer\n",
    "text_field = data.Field(\n",
    "    sequential=True,\n",
    "    init_token='<시작>', # <sos>\n",
    "    eos_token='<끝>', # <eos>\n",
    "    pad_token='<패딩>', # <pad>\n",
    "    tokenize=tokenizer,\n",
    "    use_vocab=True,\n",
    "    include_lengths=True,\n",
    "    batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:00:10.068225Z",
     "start_time": "2017-07-30T20:00:10.037487Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "examples = []\n",
    "for q, a in QA_list:\n",
    "    example = data.Example.fromlist(\n",
    "        data=[q, a],\n",
    "        fields=[('question', text_field),\n",
    "                ('answer', text_field)])\n",
    "    examples.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:00:10.096981Z",
     "start_time": "2017-07-30T20:00:10.069885Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torchtext.data.example.Example at 0x10f183630>,\n",
       " <torchtext.data.example.Example at 0x10f1836a0>,\n",
       " <torchtext.data.example.Example at 0x10f183668>,\n",
       " <torchtext.data.example.Example at 0x10f183780>,\n",
       " <torchtext.data.example.Example at 0x10f1837b8>,\n",
       " <torchtext.data.example.Example at 0x10f183978>,\n",
       " <torchtext.data.example.Example at 0x10f183a58>,\n",
       " <torchtext.data.example.Example at 0x10f1837f0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:00:10.125761Z",
     "start_time": "2017-07-30T20:00:10.099255Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:00:10.193160Z",
     "start_time": "2017-07-30T20:00:10.128176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': ['안녕', '하', '세요', '!'],\n",
      " 'question': ['안녕', '하', '세요', '?']}\n",
      "{'answer': ['비', '가', '오', '네요', '.'],\n",
      " 'question': ['오늘', '날씨', '가', '어떤', '가요', '?']}\n",
      "{'answer': ['네', ',', '헤이즐넛', '라떼', '를', '좋', '아',\n",
      "            '해요', '.'],\n",
      " 'question': ['커피', '좋', '아', '하', '세요', '?']}\n",
      "{'answer': ['자', '야죠', '뭐', 'ㅎ', 'ㅎ', '보통', '집',\n",
      "            '에', '오래', '안', '있', '어요'],\n",
      " 'question': ['집', '에서', '는', '주로', '뭘', '하', '시',\n",
      "              '나요', '?']}\n",
      "{'answer': ['저', '는', '서울', '살아요'],\n",
      " 'question': ['야', '너', '어디', '사', '냐']}\n",
      "{'answer': ['죄송', '하', '지만', '그런', '질문', '에', '는',\n",
      "            '대답', '할', '수', '없', '습니다'],\n",
      " 'question': ['여기', '자장면', '배달', '되', '나요']}\n",
      "{'answer': ['좀', '오래', '된', '거', '긴', '한데', '빅뱅',\n",
      "            '이론', '좋', '아', '해요', '.'],\n",
      " 'question': ['너', '혹시', '뭐', '좋', '아', '하', '는',\n",
      "              '드라마', '있', '어', '?']}\n",
      "{'answer': ['영화', '를', '본', '지', '가', '좀', '오래',\n",
      "            '되', '었', '는데', '혹시', '요즘', '뭐', '재밌',\n",
      "            '는', '영화', '가', '있', '나요', '?'],\n",
      " 'question': ['영화', '보', '러', '가', '자', '.']}\n"
     ]
    }
   ],
   "source": [
    "for ex in examples:\n",
    "    pprint(vars(ex), compact=True, width=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:00:10.230132Z",
     "start_time": "2017-07-30T20:00:10.199036Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_pred(example):\n",
    "    if len(example.question) > 2 and len(example.answer) > 2:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:00:10.275248Z",
     "start_time": "2017-07-30T20:00:10.243693Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QA_dataset = data.Dataset(\n",
    "    examples=examples,\n",
    "#     sort_key=lambda x: len(x.question),\n",
    "    fields=[\n",
    "        ('question', text_field),\n",
    "        ('answer', text_field)\n",
    "    ],\n",
    "    filter_pred=filter_pred\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:00:10.306679Z",
     "start_time": "2017-07-30T20:00:10.277274Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_field.build_vocab(QA_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:00:10.357181Z",
     "start_time": "2017-07-30T20:00:10.308620Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = text_field.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:00:10.399859Z",
     "start_time": "2017-07-30T20:00:10.359377Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utility fucntion for idx <-> token\n",
    "- list of ids => list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:00:10.461303Z",
     "start_time": "2017-07-30T20:00:10.411485Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ids2token(ids):\n",
    "    return [vocab.itos[id] for id in ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BucketIterator\n",
    "- Defines an iterator that batches examples of similar lengths together.\n",
    "    Minimizes amount of padding needed while producing freshly shuffled\n",
    "    batches for each new epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:00:10.508410Z",
     "start_time": "2017-07-30T20:00:10.467822Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_loader = data.BucketIterator(\n",
    "    dataset=QA_dataset,\n",
    "    sort_key = lambda ex: data.interleave_keys(len(ex.question), len(ex.answer)),\n",
    "    batch_size=2,\n",
    "    device=-1,\n",
    "    train=True,\n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:00:10.570491Z",
     "start_time": "2017-07-30T20:00:10.510783Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "Question in word indices: (Variable containing:\n",
      "    2    21     5    13     4     3     1\n",
      "    2    58    18    61    51    35     3\n",
      "[torch.LongTensor of size 2x7]\n",
      ", \n",
      " 6\n",
      " 7\n",
      "[torch.LongTensor of size 2]\n",
      ")\n",
      "\n",
      "Answer in word indices: (Variable containing:\n",
      "  2  21   5  13  28   3\n",
      "  2  74   7  53  52   3\n",
      "[torch.LongTensor of size 2x6]\n",
      ", \n",
      " 6\n",
      " 6\n",
      "[torch.LongTensor of size 2]\n",
      ")\n",
      "\n",
      "(\"Question in text: [['<시작>', '안녕', '하', '세요', '?', '<끝>', '<패딩>'], ['<시작>', \"\n",
      " \"'야', '너', '어디', '사', '냐', '<끝>']]\")\n",
      "(\"Answer in text: [['<시작>', '안녕', '하', '세요', '!', '<끝>'], ['<시작>', '저', '는', \"\n",
      " \"'서울', '살아요', '<끝>']]\")\n",
      "Batch 1\n",
      "Question in word indices: (Variable containing:\n",
      "    2    18    27    12    10     9     5     7    41    16    60     4     3\n",
      "    2    14    46    43     6    23     8     3     1     1     1     1     1\n",
      "[torch.LongTensor of size 2x13]\n",
      ", \n",
      " 13\n",
      "  8\n",
      "[torch.LongTensor of size 2]\n",
      ")\n",
      "\n",
      "Answer in word indices: (Variable containing:\n",
      "\n",
      "Columns 0 to 12 \n",
      "    2    24    15    40    31    33    81    50    71    10     9    26     8\n",
      "    2    14    20    48    77     6    24    15    19    65    38    27    70\n",
      "\n",
      "Columns 13 to 21 \n",
      "    3     1     1     1     1     1     1     1     1\n",
      "   12    73     7    14     6    16    11     4     3\n",
      "[torch.LongTensor of size 2x22]\n",
      ", \n",
      " 14\n",
      " 22\n",
      "[torch.LongTensor of size 2]\n",
      ")\n",
      "\n",
      "(\"Question in text: [['<시작>', '너', '혹시', '뭐', '좋', '아', '하', '는', '드라마', '있', \"\n",
      " \"'어', '?', '<끝>'], ['<시작>', '영화', '보', '러', '가', '자', '.', '<끝>', '<패딩>', \"\n",
      " \"'<패딩>', '<패딩>', '<패딩>', '<패딩>']]\")\n",
      "(\"Answer in text: [['<시작>', '좀', '오래', '된', '거', '긴', '한데', '빅뱅', '이론', '좋', \"\n",
      " \"'아', '해요', '.', '<끝>', '<패딩>', '<패딩>', '<패딩>', '<패딩>', '<패딩>', '<패딩>', \"\n",
      " \"'<패딩>', '<패딩>'], ['<시작>', '영화', '를', '본', '지', '가', '좀', '오래', '되', '었', \"\n",
      " \"'는데', '혹시', '요즘', '뭐', '재밌', '는', '영화', '가', '있', '나요', '?', '<끝>']]\")\n",
      "Batch 2\n",
      "Question in word indices: (Variable containing:\n",
      "    2    67    72    45    19    11     3     1     1     1     1\n",
      "    2    25    66     7    76    44     5    56    11     4     3\n",
      "[torch.LongTensor of size 2x11]\n",
      ", \n",
      "  7\n",
      " 11\n",
      "[torch.LongTensor of size 2]\n",
      ")\n",
      "\n",
      "Answer in word indices: (Variable containing:\n",
      "\n",
      "Columns 0 to 12 \n",
      "    2    75     5    78    32    79    22     7    39    82    54    64    55\n",
      "    2    23    59    12    17    17    47    25    22    15    57    16    63\n",
      "\n",
      "Columns 13 to 13 \n",
      "    3\n",
      "    3\n",
      "[torch.LongTensor of size 2x14]\n",
      ", \n",
      " 14\n",
      " 14\n",
      "[torch.LongTensor of size 2]\n",
      ")\n",
      "\n",
      "(\"Question in text: [['<시작>', '여기', '자장면', '배달', '되', '나요', '<끝>', '<패딩>', \"\n",
      " \"'<패딩>', '<패딩>', '<패딩>'], ['<시작>', '집', '에서', '는', '주로', '뭘', '하', '시', '나요', \"\n",
      " \"'?', '<끝>']]\")\n",
      "(\"Answer in text: [['<시작>', '죄송', '하', '지만', '그런', '질문', '에', '는', '대답', '할', \"\n",
      " \"'수', '없', '습니다', '<끝>'], ['<시작>', '자', '야죠', '뭐', 'ㅎ', 'ㅎ', '보통', '집', '에', \"\n",
      " \"'오래', '안', '있', '어요', '<끝>']]\")\n",
      "Batch 3\n",
      "Question in word indices: (Variable containing:\n",
      "    2    69    34     6    62    30     4     3\n",
      "    2    80    10     9     5    13     4     3\n",
      "[torch.LongTensor of size 2x8]\n",
      ", \n",
      " 8\n",
      " 8\n",
      "[torch.LongTensor of size 2]\n",
      ")\n",
      "\n",
      "Answer in word indices: (Variable containing:\n",
      "    2    49     6    68    37     8     3     1     1     1     1\n",
      "    2    36    29    83    42    20    10     9    26     8     3\n",
      "[torch.LongTensor of size 2x11]\n",
      ", \n",
      "  7\n",
      " 11\n",
      "[torch.LongTensor of size 2]\n",
      ")\n",
      "\n",
      "(\"Question in text: [['<시작>', '오늘', '날씨', '가', '어떤', '가요', '?', '<끝>'], \"\n",
      " \"['<시작>', '커피', '좋', '아', '하', '세요', '?', '<끝>']]\")\n",
      "(\"Answer in text: [['<시작>', '비', '가', '오', '네요', '.', '<끝>', '<패딩>', '<패딩>', \"\n",
      " \"'<패딩>', '<패딩>'], ['<시작>', '네', ',', '헤이즐넛', '라떼', '를', '좋', '아', '해요', '.', \"\n",
      " \"'<끝>']]\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmin/anaconda/envs/mldemo/lib/python3.6/site-packages/ipykernel/__main__.py:1: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "for batch_i, batch in enumerate(data_loader):\n",
    "    print('Batch', batch_i)\n",
    "    question, answer = batch.question, batch.answer\n",
    "    \n",
    "    print(f'Question in word indices: {question}\\n')\n",
    "    print(f'Answer in word indices: {answer}\\n')\n",
    "    \n",
    "    # padded variable, list of lengths of batches\n",
    "    question_var, question_len = question\n",
    "    answer_var, answer_len = answer\n",
    "    \n",
    "    pprint(f'Question in text: {[ids2token(batch) for batch in question_var.data.numpy()]}')\n",
    "    pprint(f'Answer in text: {[ids2token(batch) for batch in answer_var.data.numpy()]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:00:10.621344Z",
     "start_time": "2017-07-30T20:00:10.572894Z"
    },
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = datasets.TranslationDataset.splits(\n",
    "    path='./datasets/',\n",
    "    train='train_',\n",
    "    validation='valid_',\n",
    "    test='test_',\n",
    "    exts=('question.txt', 'answer.txt'),\n",
    "    fields=(text_field, text_field),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:00:10.673965Z",
     "start_time": "2017-07-30T20:00:10.623284Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.datasets.translation.TranslationDataset at 0x122d50080>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:00:10.711387Z",
     "start_time": "2017-07-30T20:00:10.676107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': ['안녕', '하', '세요', '?'], 'trg': ['안녕', '하', '세요', '!']}\n",
      "{'src': ['오늘', '날씨', '가', '어떤', '가요', '?'], 'trg': ['비', '가', '오', '네요', '.']}\n",
      "{'src': ['커피', '좋', '아', '하', '세요', '?'], 'trg': ['네', ',', '헤이즐넛', '라떼', '를', '좋', '아', '해요', '.']}\n",
      "{'src': ['집', '에서', '는', '주로', '뭘', '하', '시', '나요', '?'], 'trg': ['자', '야죠', '뭐', 'ㅎ', 'ㅎ', '보통', '집', '에', '오래', '안', '있', '어요']}\n"
     ]
    }
   ],
   "source": [
    "for ex in train_data:\n",
    "    print(vars(ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:00:10.772322Z",
     "start_time": "2017-07-30T20:00:10.713117Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_iter = data.BucketIterator(\n",
    "    dataset=train_data,\n",
    "    sort_key = lambda ex: data.interleave_keys(len(ex.src), len(ex.trg)), # \n",
    "    batch_size=2,\n",
    "    device=-1,\n",
    "    train=True,\n",
    "    repeat=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:00:10.841590Z",
     "start_time": "2017-07-30T20:00:10.782524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch  0\n",
      "Question in word indices: (Variable containing:\n",
      "    2    80    10     9     5    13     4     3     1     1     1\n",
      "    2    25    66     7    76    44     5    56    11     4     3\n",
      "[torch.LongTensor of size 2x11]\n",
      ", \n",
      "  8\n",
      " 11\n",
      "[torch.LongTensor of size 2]\n",
      ")\n",
      "\n",
      "Answer in word indices: (Variable containing:\n",
      "\n",
      "Columns 0 to 12 \n",
      "    2    36    29    83    42    20    10     9    26     8     3     1     1\n",
      "    2    23    59    12    17    17    47    25    22    15    57    16    63\n",
      "\n",
      "Columns 13 to 13 \n",
      "    1\n",
      "    3\n",
      "[torch.LongTensor of size 2x14]\n",
      ", \n",
      " 11\n",
      " 14\n",
      "[torch.LongTensor of size 2]\n",
      ")\n",
      "\n",
      "(\"Question in text: [['<시작>', '커피', '좋', '아', '하', '세요', '?', '<끝>', '<패딩>', \"\n",
      " \"'<패딩>', '<패딩>'], ['<시작>', '집', '에서', '는', '주로', '뭘', '하', '시', '나요', '?', \"\n",
      " \"'<끝>']]\")\n",
      "(\"Answer in text: [['<시작>', '네', ',', '헤이즐넛', '라떼', '를', '좋', '아', '해요', '.', \"\n",
      " \"'<끝>', '<패딩>', '<패딩>', '<패딩>'], ['<시작>', '자', '야죠', '뭐', 'ㅎ', 'ㅎ', '보통', \"\n",
      " \"'집', '에', '오래', '안', '있', '어요', '<끝>']]\")\n",
      "batch  1\n",
      "Question in word indices: (Variable containing:\n",
      "    2    21     5    13     4     3     1     1\n",
      "    2    69    34     6    62    30     4     3\n",
      "[torch.LongTensor of size 2x8]\n",
      ", \n",
      " 6\n",
      " 8\n",
      "[torch.LongTensor of size 2]\n",
      ")\n",
      "\n",
      "Answer in word indices: (Variable containing:\n",
      "    2    21     5    13    28     3     1\n",
      "    2    49     6    68    37     8     3\n",
      "[torch.LongTensor of size 2x7]\n",
      ", \n",
      " 6\n",
      " 7\n",
      "[torch.LongTensor of size 2]\n",
      ")\n",
      "\n",
      "(\"Question in text: [['<시작>', '안녕', '하', '세요', '?', '<끝>', '<패딩>', '<패딩>'], \"\n",
      " \"['<시작>', '오늘', '날씨', '가', '어떤', '가요', '?', '<끝>']]\")\n",
      "(\"Answer in text: [['<시작>', '안녕', '하', '세요', '!', '<끝>', '<패딩>'], ['<시작>', \"\n",
      " \"'비', '가', '오', '네요', '.', '<끝>']]\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmin/anaconda/envs/mldemo/lib/python3.6/site-packages/ipykernel/__main__.py:1: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "for batch_i, batch in enumerate(train_data_iter):\n",
    "    print('batch ', batch_i)\n",
    "    question, answer = batch.src, batch.trg\n",
    "    \n",
    "    print(f'Question in word indices: {question}\\n')\n",
    "    print(f'Answer in word indices: {answer}\\n')\n",
    "    \n",
    "    # padded variable, list of lengths of batches\n",
    "    question_var, question_len = question\n",
    "    answer_var, answer_len = answer\n",
    "    \n",
    "    pprint(f'Question in text: {[ids2token(batch) for batch in question_var.data.numpy()]}')\n",
    "    pprint(f'Answer in text: {[ids2token(batch) for batch in answer_var.data.numpy()]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually made vocabulary class (for more complex data that are not easily handled with torchtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:00:10.909595Z",
     "start_time": "2017-07-30T20:00:10.844453Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokenizer=twitter.morphs):\n",
    "        self.idx2word = []\n",
    "        self.word2idx = {}\n",
    "        self.vocab_size = 0\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        try:\n",
    "            assert isinstance(word, str)\n",
    "            if word not in self.word2idx:\n",
    "                self.idx2word.append(word)\n",
    "                self.word2idx[word] = self.vocab_size\n",
    "                self.vocab_size += 1\n",
    "            \n",
    "        except AssertionError:\n",
    "            print('Input should be str')\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        words = self.tokenizer(sentence)\n",
    "        for word in words:\n",
    "            self.add_word(word)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:00:10.939336Z",
     "start_time": "2017-07-30T20:00:10.913193Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "manual_vocab = Vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:00:12.523752Z",
     "start_time": "2017-07-30T20:00:10.941182Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for q, a in QA_list:\n",
    "    manual_vocab.add_sentence(q)\n",
    "    manual_vocab.add_sentence(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-30T20:00:12.559217Z",
     "start_time": "2017-07-30T20:00:12.526292Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx2word': ['안녕하세',\n",
       "  '요',\n",
       "  '?',\n",
       "  '!',\n",
       "  '오늘',\n",
       "  '날씨',\n",
       "  '가',\n",
       "  '어떤',\n",
       "  '가요',\n",
       "  '비',\n",
       "  '오네',\n",
       "  '.',\n",
       "  '커피',\n",
       "  '좋아하세',\n",
       "  '네',\n",
       "  ',',\n",
       "  '헤이즐넛',\n",
       "  '라떼',\n",
       "  '를',\n",
       "  '좋아해',\n",
       "  '집',\n",
       "  '에서는',\n",
       "  '주로',\n",
       "  '뭘',\n",
       "  '하시',\n",
       "  '나요',\n",
       "  '자야',\n",
       "  '죠',\n",
       "  '뭐',\n",
       "  'ㅎㅎ',\n",
       "  '보통',\n",
       "  '에',\n",
       "  '오래',\n",
       "  '안',\n",
       "  '있어',\n",
       "  '야',\n",
       "  '너',\n",
       "  '어디',\n",
       "  '사냐',\n",
       "  '저',\n",
       "  '는',\n",
       "  '서울',\n",
       "  '살',\n",
       "  '아요',\n",
       "  '여기',\n",
       "  '자장면',\n",
       "  '배달',\n",
       "  '되',\n",
       "  '죄송하지',\n",
       "  '만',\n",
       "  '그런',\n",
       "  '질문',\n",
       "  '에는',\n",
       "  '대답할',\n",
       "  '수',\n",
       "  '없',\n",
       "  '습니다',\n",
       "  '혹시',\n",
       "  '좋아하는',\n",
       "  '드라마',\n",
       "  '좀',\n",
       "  '오래된',\n",
       "  '거',\n",
       "  '긴',\n",
       "  '한데',\n",
       "  '빅뱅이론',\n",
       "  '영화',\n",
       "  '보러',\n",
       "  '자',\n",
       "  '본',\n",
       "  '지',\n",
       "  '되었',\n",
       "  '는데',\n",
       "  '요즘',\n",
       "  '재밌는',\n",
       "  '있'],\n",
       " 'tokenizer': <bound method Twitter.morphs of <konlpy.tag._twitter.Twitter object at 0x10eeab0f0>>,\n",
       " 'vocab_size': 76,\n",
       " 'word2idx': {'!': 3,\n",
       "  ',': 15,\n",
       "  '.': 11,\n",
       "  '?': 2,\n",
       "  'ㅎㅎ': 29,\n",
       "  '가': 6,\n",
       "  '가요': 8,\n",
       "  '거': 62,\n",
       "  '그런': 50,\n",
       "  '긴': 63,\n",
       "  '나요': 25,\n",
       "  '날씨': 5,\n",
       "  '너': 36,\n",
       "  '네': 14,\n",
       "  '는': 40,\n",
       "  '는데': 72,\n",
       "  '대답할': 53,\n",
       "  '되': 47,\n",
       "  '되었': 71,\n",
       "  '드라마': 59,\n",
       "  '라떼': 17,\n",
       "  '를': 18,\n",
       "  '만': 49,\n",
       "  '뭐': 28,\n",
       "  '뭘': 23,\n",
       "  '배달': 46,\n",
       "  '보러': 67,\n",
       "  '보통': 30,\n",
       "  '본': 69,\n",
       "  '비': 9,\n",
       "  '빅뱅이론': 65,\n",
       "  '사냐': 38,\n",
       "  '살': 42,\n",
       "  '서울': 41,\n",
       "  '수': 54,\n",
       "  '습니다': 56,\n",
       "  '아요': 43,\n",
       "  '안': 33,\n",
       "  '안녕하세': 0,\n",
       "  '야': 35,\n",
       "  '어디': 37,\n",
       "  '어떤': 7,\n",
       "  '없': 55,\n",
       "  '에': 31,\n",
       "  '에는': 52,\n",
       "  '에서는': 21,\n",
       "  '여기': 44,\n",
       "  '영화': 66,\n",
       "  '오네': 10,\n",
       "  '오늘': 4,\n",
       "  '오래': 32,\n",
       "  '오래된': 61,\n",
       "  '요': 1,\n",
       "  '요즘': 73,\n",
       "  '있': 75,\n",
       "  '있어': 34,\n",
       "  '자': 68,\n",
       "  '자야': 26,\n",
       "  '자장면': 45,\n",
       "  '재밌는': 74,\n",
       "  '저': 39,\n",
       "  '좀': 60,\n",
       "  '좋아하는': 58,\n",
       "  '좋아하세': 13,\n",
       "  '좋아해': 19,\n",
       "  '죄송하지': 48,\n",
       "  '죠': 27,\n",
       "  '주로': 22,\n",
       "  '지': 70,\n",
       "  '질문': 51,\n",
       "  '집': 20,\n",
       "  '커피': 12,\n",
       "  '하시': 24,\n",
       "  '한데': 64,\n",
       "  '헤이즐넛': 16,\n",
       "  '혹시': 57}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(manual_vocab)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:mldemo]",
   "language": "python",
   "name": "conda-env-mldemo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
